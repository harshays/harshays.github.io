<!doctype html>
<html lang="en">
  <head>
    <!-- meta  -->
    <meta type="author" content="Harshay Shah"/>
    <meta charset="utf-8" http-eqiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, shrink-to-fit=no">

    <!-- cdn + font -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css?family=Gentium+Basic:400,700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- css/js -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
    <link rel="stylesheet" type="text/css" href="./css/all.min.css">
    <link rel="stylesheet" type="text/css" href="./css/style.css">

    <script src="./js/jquery-2.2.4.min.js"></script>
    <script src="./js/app.js"></script>
    <script src="./js/scramble.js"></script>

    <title>Harshay Shah</title>
  </head>

  <body>
    <div class="container">

      <!-- XS Header -->
      <div class="row header-row">
              <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
              <div class="col-lg-8 col-md-10 col-sm-12 header-column">
                <div class="row">

                  <div class="col-xs-4 img-xs text-center hidden-sm hidden-md hidden-lg">
                    <img src="resources/me.png" class="img-responsive">
                  </div>

                  <!-- <div class="col-sm-12 col-xs-8 hidden-sm hidden-md hidden-lg"> -->
                  <div class="col-sm-12 col-xs-8">
                    <div class="row">
                      <div class="col-sm-6 header-inner-col hidden-sm hidden-md hidden-lg">
                          <p class="header-xs">Harshay <span class="hidden-xxs">Shah</span></p>
                          <p class="bio-subheader-xs hidden-xxs">PhD Student at MIT EECS</p>
                      </div>

                      <div class="col-sm-6 header-right">
                        <div class="social-links-xs text-left hidden-sm hidden-md hidden-lg">
                          <!-- <a class="icons" href="./resources/cv.pdf" target="_blank"><i class="ai ai-cv-square ai-1x"></i></a> -->
                          <!-- <a class="icons" href="mailto:harshay@mit.edu" target="_blank"><i class="fas fa-envelope-square fa-1x"></i></a> -->
                          <a class="icons" href="https://twitter.com/harshays_" target="_blank"><i class="fab fa-twitter-square fa-1x"></i></a>
                          <a class="icons" href="https://github.com/harshays" target="_blank"><i class="fab fa-github-square fa-1x"></i></a>
                          <a class="icons" href="https://scholar.google.com/citations?hl=en&user=oC8YKjUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><i class="ai ai-google-scholar-square ai-1x"></i></a>
                          <a class="icons" href="https://linkedin.com/in/harshayshah" target="_blank"><i class="fab fa-linkedin fa-1x"></i></a>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <!-- Non XS header -->
      <div class="content-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
        <div class="col-lg-8 col-md-10 col-xs-12">
          <div class="content-inner-row row">

            <div class="col-sm-3 hidden-xs img-col text-right">
              <div class="img-div">
                <img src="resources/me.png" alt="my face" class="img-md hidden-xs img-responsive">
              </div>
            </div>

            <div class="col-xs-12 col-sm-9 content-col">
              <p class="header hidden-xs">Harshay <span class="last-name">Shah</span></p>
              <p class="bio-subheader hidden-xs">PhD Student at MIT EECS</p>

              <div class="social-links text-left hidden-xs">
                <!-- <a class="icons" href="./resources/cv.pdf" target="_blank"><i class="ai ai-cv-square ai-1x"></i></a> -->
                <a class="icons" id="emailicon2"><i class="fas fa-envelope-square fa-1x"></i></a>
                <script>emailIconScramble = new scrambledString(document.getElementById('emailicon2'),'emailIconScramble', '@auhey.tamsdrih', [7, 1, 14, 0, 12, 6, 11, 10, 5, 8, 3, 13, 2, 9, 4], '<i class="fas fa-envelope-square fa-1x"></i>');</script>
                <a class="icons" href="https://twitter.com/harshays_" target="_blank"><i class="fab fa-twitter-square fa-1x"></i></a>
                <a class="icons" href="https://github.com/harshays" target="_blank"><i class="fab fa-github-square fa-1x"></i></a>
                <a class="icons" href="https://scholar.google.com/citations?hl=en&user=oC8YKjUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><i class="ai ai-google-scholar-square ai-1x"></i></a>
                <a class="icons" href="https://linkedin.com/in/harshayshah" target="_blank"><i class="fab fa-linkedin fa-1x"></i></a>
              </div>
            </div>


          </div>
          <!-- <hr> -->
        </div>
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <div id="about" class="about-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
        <div class="col-lg-8 col-md-10 col-sm-12">
          <!-- <div class="subheader-row row">
            <div class="col-xs-12 subheader-col">
              <p class="subheader">About</p>
            </div>
          </div> -->

          <div class="about-content-row row">
            <div class="col-xs-12">
              <div class="about-content-div">
                <p>
                  I am a PhD student at <a target="_blank" href="https://www.eecs.mit.edu/">MIT</a>, where I am advised by <a target="_blank" href="https://madrylab.mit.edu/">Aleksander Mądry</a>.
                  Previously, I spent two great years at Microsoft Research with <a target="_blank" href="http://praneethnetrapalli.org/">Praneeth Netrapalli</a> and
                  <a target="_blank" href="http://www.prateekjain.org/">Prateek Jain</a>.
                  Before that, I studied CS and Stats at <a target="_blank" href="https://cs.illinois.edu/">UIUC</a>.
                  I have interned at Apple MLR, Google Research, and Akuna Capital.
                </p>
                <p>
                  I am broadly interested in understanding and steering large-scale machine learning models.
                  My recent work focuses on developing tools for analyzing model behavior via targeted interventions to
                  <a href="#moe">learning algorithms</a>,
                  <a href="#modeldiff">training data</a>,
                  <a href="#contextcite">in-context information</a>,
                  and <a href="#modelcomponents">learned representations</a>.
                  Outside of research, I enjoy cricket and tennis.
                </p>
              </div>
            </div>
          </div>
          <hr>
        </div>
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <!-- Papers -->
      <div id="paper" class="paper-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>

        <div class="col-lg-8 col-md-10 col-sm-12">
          <div class="subheader-row row">
            <div class="col-xs-12 subheader-col">
                <p class="subheader">Papers</p>
                <!-- <p class="subheader">Selected Papers <span class="selected-button-span">(<a id="selected-button">Show all</a>)</span></p> -->

            </div>
          </div>
          <div class="papers-content-row row">
            <div class="col-xs-12 papers-content-col">


              <!-- MoE -->
              <div class="paper-div" id="moe">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2501.12370">Parameters vs FLOPs: Scaling Laws for Optimal Sparsity of MoE Language Models</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors">Samira Abnar*, <span class="underline">Harshay Shah*</span>, Dan Busbridge, Alaa El-Nouby, Josh Susskind, Vimal Thilak*</span><br/>
                  <span class="paper-venue paper-venue-workshop">Workshop on Attributing Model Behavior at Scale</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://attrib-workshop.cc/">NeurIPS Attrib</a>)</span>, 2024</span>
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah</span>, V. Thilak, D. Busbridge, A. El-Nouby, J. Susskind, S. Abnar</span><br/>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank"  href="https://attrib-workshop.cc/">NeurIPS Attrib</a></span>, 2024</span>
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2501.12370" target="_blank" class="paper-button">arxiv</a>
                  <a id="p8" class="paper-button abstract-button">abstract</a>
                </div>

                <div id="abs8" class="paper-abstract" style="display: none;">
                <blockquote>
                    Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Expert models (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the ratio of non-active to total parameters, affects model performance in terms of both pretraining and downstream performance. We find that under different constraints (e.g. parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.
                </blockquote>
                </div>

                <div id="bib8" style="display: none;">
                    <pre class="language-bib" class="paper-bib" data-lang>
@misc{abnar2025parameters,
    title={Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models},
    author={Samira Abnar and Harshay Shah and Dan Busbridge and Alaaeldin Mohamed Elnouby Ali and Josh Susskind and Vimal Thilak},
    year={2025},
    eprint={2501.12370},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
                    </div>
              </div>

              <!-- ContextCite -->
              <div class="paper-div" id="contextcite">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2409.00729">ContextCite: Attributing Model Generation to Context</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors">Benjamin Cohen-Wang*, <span class="underline">Harshay Shah*</span>, Kristian Georgiev*, Aleksander Mądry</span><br/>
                  <span class="paper-venue">Neural Information Processing Systems</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://neurips.cc/Conferences/2024">NeurIPS</a>)</span>, 2024<br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Next Generation AI Safety</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://icml-nextgenaisafety.github.io/">ICML NextGenAISafety</a>)</span>, 2024</span>
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors">B. Cohen-Wang*, <span class="underline">H. Shah*</span>, K. Georgiev*, A. Mądry</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a></span><br>
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://icml-nextgenaisafety.github.io/">ICML NextGenAISafety</a></span>, 2024</span> -->
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2409.00729" target="_blank" class="paper-button">arxiv</a>
                  <a id="p7" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/MadryLab/context-cite"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_contextcite.pdf"  target="_blank" class="paper-button">poster</a>
                  <a href="https://gradientscience.org/contextcite/" target="_blank" class="paper-button">blog post</a>
                  <a href="https://news.mit.edu/2024/citation-tool-contextcite-new-approach-trustworthy-ai-generated-content-1209" target="_blank" class="paper-button">article</a>
                  <a href="https://huggingface.co/spaces/contextcite/context-cite" target="_blank" class="paper-button">demo</a>
                </div>

                <div id="abs7" class="paper-abstract" style="display: none;">
                  <blockquote>
                    How do language models actually use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through two case studies: (1) automatically verifying statements based on the attributed parts of the context and (2) improving response quality by extracting query-relevant information from the context.
                  </blockquote>
                </div>

                <div id="bib7" style="display: none;">
                    <pre class="language-bib" class="paper-bib" data-lang>
@article{cohen2024contextcite,
    title={ContextCite: Attributing Model Generation to Context},
    author={Cohen-Wang, Benjamin and Shah, Harshay and Georgiev, Kristian and Madry, Aleksander},
    journal={arXiv preprint arXiv:2409.00729},
    year={2024}
    }</pre>
                </div>
              </div>




              <!-- Model components -->
              <div class="paper-div" id="modelcomponents">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2404.11534">Decomposing and Editing Predictions by Modeling Model Computation</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Andrew Ilyas, Aleksander Mądry</span><br/>
                  <span class="paper-venue">International Conference on Machine Learning</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://icml.cc/Conferences/2024">ICML</a>)</span>, 2024<br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Foundation Model Interventions</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/mint-2024">NeurIPS MINT</a>)</span>, 2024</span>
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                  <!-- <span class="paper-venue paper-venue-workshop">+ Workshop on High-dimensional Learning Dynamics</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/hidimlearning/home">ICML HiLD</a>)</span>, 2024</span> -->
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah</span>, A. Ilyas, A. Mądry</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://icml.cc/Conferences/2024">ICML 2024</a></span><br>
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/hidimlearning/home">+ ICML HiLD</a></span>, 2024</span> -->
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2404.11534" target="_blank" class="paper-button">arxiv</a>
                  <a id="p6" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/MadryLab/modelcomponents"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_coar.pdf"  target="_blank" class="paper-button">poster</a>
                  <a href="https://gradientscience.org/modelcomponents/" target="_blank" class="paper-button">blog post</a>
                </div>

                <div id="abs6" class="paper-abstract" style="display: none;">
                  <blockquote>
                    How does the internal computation of a machine learning model transform inputs into predictions? In this paper, we introduce a task called component modeling that aims to address this question. The goal of component modeling is to decompose an ML model's prediction in terms of its components -- simple functions (e.g., convolution filters, attention heads) that are the "building blocks" of model computation. We focus on a special case of this task, component attribution, where the goal is to estimate the counterfactual impact of individual components on a given prediction. We then present COAR, a scalable algorithm for estimating component attributions; we demonstrate its effectiveness across models, datasets, and modalities. Finally, we show that component attributions estimated with COAR directly enable model editing across five tasks, namely: fixing model errors, "forgetting" specific classes, boosting subpopulation robustness, localizing backdoor attacks, and improving robustness to typographic attacks. We provide code for COAR at  <a href="https://github.com/MadryLab/modelcomponents">github.com/MadryLab/modelcomponents</a>
                  </blockquote>
                </div>

                <div id="bib6" style="display: none;">
                    <pre class="language-bib" class="paper-bib" data-lang>
@article{shah2024decomposing,
  title={Decomposing and Editing Predictions by Modeling Model Computation},
  author={Shah, Harshay and Ilyas, Andrew and Madry, Aleksander},
  journal={arXiv preprint arXiv:2404.11534},
  year={2024}
}</pre>
                </div>
              </div>

              <!-- ModelDiff -->
              <div class="paper-div" id="modeldiff">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2211.12491">ModelDiff: A Framework for Comparing Learning Algorithms</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>*, Sung Min Park*, Andrew Ilyas*, Aleksander Mądry</span><br/>
                  <span class="paper-venue">International Conference on Machine Learning</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://icml.cc/Conferences/2023">ICML</a>)</span>, 2023<br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Spurious Correlations, Invariance, and Stability</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/scis-workshop-23">ICML SCIS</a>)</span>, 2023</span>
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                  <!-- <span class="paper-venue paper-venue-workshop">NeurIPS Workshop on Distribution Shifts</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/distshift2022/home?authuser=0">NeurIPS DistShift</a>)</span>, 2022</span> -->
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah*</span>, S. M. Park*, A. Ilyas*, A. Mądry</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://icml.cc/Conferences/2023">ICML 2023</a></span><br>
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/scis-workshop-23">+ ICML SCIS</a></span>, 2023</span> -->
                  <!-- <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br> -->
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/distshift2022/home?authuser=0">NeurIPS DistShift</a></span>, 2022</span> -->
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2211.12491" target="_blank" class="paper-button">arxiv</a>
                  <a id="p5" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/MadryLab/modeldiff"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_modeldiff.pdf"  target="_blank" class="paper-button">poster</a>
                  <a href="https://gradientscience.org/modeldiff/" target="_blank" class="paper-button">blog post</a>
                </div>

                <div id="abs5" class="paper-abstract" style="display: none;">
                  <blockquote>
                    We study the problem of (learning) algorithm comparison, where the goal is to find differences between models trained with two different learning algorithms. We begin by formalizing this goal as one of finding distinguishing feature transformations, i.e., input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present ModelDiff, a method that leverages the datamodels framework (Ilyas et al., 2022) to compare learning algorithms based on how they use their training data. We demonstrate ModelDiff through three case studies, comparing models trained with/without data augmentation, with/without pre-training, and with different SGD hyperparameters. Our code is available at <a href="https://github.com/MadryLab/modeldiff">github.com/MadryLab/modeldiff</a>
                  </blockquote>
                </div>

                <div id="bib5" style="display: none;">
                    <pre class="language-bib" class="paper-bib" data-lang>
@inproceedings{shah2023modeldiff,
    title={Modeldiff: A framework for comparing learning algorithms},
    author={Shah, Harshay and Park, Sung Min and Ilyas, Andrew and Madry, Aleksander},
    booktitle={International Conference on Machine Learning},
    pages={30646--30688},
    year={2023},
    organization={PMLR}
}</pre>
                </div>
              </div>


              <!-- Input Gradients -->
              <div class="paper-div" id="inputgrad">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2102.12781">Do Input Gradients Highlight Discriminative Features?</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Prateek Jain, Praneeth Netrapalli</span><br/>
                  <span class="paper-venue">Neural Information Processing Systems</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://neurips.cc/Conferences/2020">NeurIPS</a>)</span>, 2021<br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Science and Engineering of Deep Learning</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/sedl-workshop/past-editions/2021?authuser=0">ICLR SEDL</a>)</span>, 2021</span>
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Responsible AI</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/rai-workshop/">ICLR RAI</a>)</span>, 2021</span>
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah</span>, P. Jain, P. Netrapalli</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://neurips.cc/Conferences/2021">NeurIPS 2021</a></span><br>
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/sedl-workshop/past-editions/2021?authuser=0">+ ICLR SEDL</a></span>, 2021</span> -->
                  <!-- <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br> -->
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/rai-workshop/">+ ICLR RAI</a></span>, 2021</span> -->
                  <!-- <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br> -->
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2102.12781" target="_blank" class="paper-button">arxiv</a>
                  <a id="p4" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/inputgradients"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_neurips21_inputgrad.pdf"  target="_blank" class="paper-button">poster</a>
                </div>

                <div id="abs4" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients -- gradients of logits with respect to input -- noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach. First, we develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A). Second, we introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A). Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at <a href="https://github.com/harshays/inputgradients">github.com/harshays/inputgradients</a>.
                  </blockquote>
                </div>

                <div id="bib4" style="display: none;">
                  <pre class="language-bib" class="paper-bib" data-lang>
@article{shah2021input,
    title={Do Input Gradients Highlight Discriminative Features?},
    author={Shah, Harshay and Jain, Prateek and Netrapalli, Praneeth},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    year={2021}
}</pre>
                </div>
              </div>

              <!-- Simplicity Bias -->
              <div class="paper-div" id="si">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2006.07710v2">The Pitfalls of Simplicity Bias in Neural Networks</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli</span><br/>
                  <span class="paper-venue">Neural Information Processing Systems</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://neurips.cc/Conferences/2020">NeurIPS</a>)</span>, 2020<br>
                  <span class="paper-venue paper-venue-workshop">+ Workshop on Uncertainty and Robustness in Deep Learning</span>
                  <span class="paper-venue-workshop"><span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/udlworkshop2020/home">ICML UDL</a>)</span>, 2020<br>
                </span>
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah</span>, K. Tamuly, A. Raghunathan, P. Jain, P. Netrapalli</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://neurips.cc/Conferences/2020">NeurIPS 2020</a></span><br>
                  <!-- <span class="paper-venue-workshop"><span class="paper-venue-abbrv"><a target="_blank" href="https://sites.google.com/view/udlworkshop2020/home">+ ICML UDL</a></span>, 2020<br> -->
                </span>
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2006.07710v2" target="_blank" class="paper-button">arxiv</a>
                  <a id="p3" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/simplicitybiaspitfalls"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_neurips20_simplicitybias.pdf"  target="_blank" class="paper-button">poster</a>
                </div>

                <div id="abs3" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Several works have proposed Simplicity Bias (SB)&mdash;the tendency of standard training procedures such as Stochastic Gradient Descent (SGD) to find simple models&mdash;to justify why neural networks generalize well [Arpit et al. 2017, Nakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of simplicity remains vague. Furthermore, previous settings that use SB to theoretically justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks&mdash;a widely observed phenomenon in practice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile SB and the superior standard generalization of neural networks with the non-robustness observed in practice by designing datasets that (a) incorporate a precise notion of simplicity, (b) comprise multiple predictive features with varying levels of simplicity, and (c) capture the non-robustness of neural networks trained on real data. Through theory and empirics on these datasets, we make four observations: (i) SB of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features. (ii) The extreme aspect of SB could explain why seemingly benign distribution shifts and small adversarial perturbations significantly degrade model performance. (iii) Contrary to conventional wisdom, SB can also hurt generalization on the same data distribution, as SB persists even when the simplest feature has less predictive power than the more complex features. (iv) Common approaches to improve generalization and robustness&mdash;ensembles and adversarial training&mdash;can fail in mitigating SB and its pitfalls. Given the role of SB in training neural networks, we hope that the proposed datasets and methods serve as an effective testbed to evaluate novel algorithmic approaches aimed at avoiding the pitfalls of SB;  code and data available at <a href="https://github.com/harshays/simplicitybiaspitfalls/">github.com/harshays/simplicitybiaspitfalls</a>.
                  </blockquote>
                </div>

                <div id="bib3" style="display: none;">
                  <pre class="language-bib" class="paper-bib" data-lang>
@article{shah2020pitfalls,
    title={The Pitfalls of Simplicity Bias in Neural Networks},
    author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    year={2020}
}</pre>
                </div>
              </div>

              <!-- ARW -->
              <div class="paper-div paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/1712.10195">Growing Attributed Networks through Local Processes</a>

                <p class="paper-info hidden-xs">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Suhansanu Kumar, Hari Sundaram</span><br/>
                  <span class="paper-venue">World Wide Web Conference </span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://www2019.thewebconf.org/">WWW</a>)</span>, 2019
                </p>

                <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors"><span class="underline">H. Shah</span>, S. Kumar, H. Sundaram</span><br/>
                  <span class="paper-venue-abbrv"><a target="_blank" href="https://www2019.thewebconf.org/">WWW</a></span>, 2019
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/1712.10195" target="_blank" class="paper-button">arxiv</a>
                  <a id="p2" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/CrowdDynamicsLab/ARW"  target="_blank" class="paper-button">code</a>
                  <a href="pdf/poster_www19_arw.pdf"  target="_blank" class="paper-button">poster</a>
                  <a href="https://crowddynamicslab.github.io/networks/2019/06/06/Growing-Attributed-Networks/"  target="_blank" class="paper-button">blog post</a>
                </div>

                <div id="abs2" class="paper-abstract" style="display: none;">
                  <blockquote>
                    This paper proposes an attributed network growth model. Despite the knowledge that individuals use limited resources to form connections to similar others, we lack an understanding of how local and resource-constrained mechanisms explain the emergence of rich structural properties found in real-world networks. We make three contributions. First, we propose a parsimonious and accurate model of attributed network growth that jointly explains the emergence of in-degree distributions, local clustering, clustering-degree relationship and attribute mixing patterns. Second, our model is based on biased random walks and uses local processes to form edges without recourse to global network information. Third, we account for multiple sociological phenomena: bounded rationality, structural constraints, triadic closure, attribute homophily, and preferential attachment. Our experiments indicate that the proposed Attributed Random Walk (ARW) model accurately preserves network structure and attribute mixing patterns of six real-world networks; it improves upon the performance of eight state-of-the-art models by a statistically significant margin of 2.5-10x.
                  </blockquote>
                </div>

                <div id="bib2" style="display: none;">
                  <pre class="language-bib" class="paper-bib" data-lang>
@inproceedings{shah2019growing,
    title={Growing Attributed Networks through Local Processes},
    author={Shah, Harshay and Kumar, Suhansanu and Sundaram, Hari},
    booktitle={The World Wide Web Conference},
    pages={3208--3214},
    year={2019},
    organization={ACM}
}</pre>
                </div>
              </div>

              <!-- Connected Components -->
              <!-- <div class="paper-div paper-div"> -->
                <!-- <a class="paper-title" target="_blank" href="https://arxiv.org/abs/1812.00139">Number of Connected Components in a Graph: Estimation via Counting Patterns</a> -->

                <!-- <p class="paper-info hidden-xs">
                  <span class="paper-authors">Ashish Khetan, <span class="underline">Harshay Shah</span>, Sewoong Oh</span><br/>
                  <span class="paper-venue">Preprint: arXiv:1812.00139</span>, 2018
                </p> -->

                <!-- <p class="paper-info hidden-sm hidden-md hidden-lg">
                  <span class="paper-authors">A. Khetan, <span class="underline">H. Shah</span>, S. Oh</span><br/>
                </p> -->

                <!-- <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/1812.00139" target="_blank" class="paper-button">arxiv</a>
                  <a id="p1" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/connectedcomponents"  target="_blank" class="paper-button">code</a>
                  <a id="b1" class="paper-button bib-button">bib</a>
                </div> -->

                <!-- <div id="abs1" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Due to resource constraints and restricted access to large-scale graph datasets, it is often necessary to work with a sampled subgraph of a larger original graph. The task of inferring a global property of the original graph from the sampled subgraph is of fundamental interest. In this work, we focus on estimating the number of connected components. Due to the inherent difficulty of the problem for general graphs, little is known about the connection between the number of connected components in the observed subgraph and the original graph. To make this connection, we propose a highly redundant motif-based representation of the observed subgraph, which, at first glance, may seem counter-intuitive. However, the proposed representation is crucial in introducing a  novel estimator for the number of connected components in general graphs. The connection is made precise via the Schatten \(k\)-norms of the graph Laplacian and the spectral representation of the number of connected components.  We provide a guarantee on the resulting mean squared error that characterizes the bias-variance trade-off. Furthermore, our experiments on synthetic and real-world graphs show that we improve upon competing algorithms for graphs with spectral gaps bounded away from zero.
                  </blockquote>
                </div> -->

                <!-- <div id="bib1" style="display: none;">
                  <pre class="language-bib" class="paper-bib" data-lang>
@article{khetan2018number,
    title={Number of Connected Components in a Graph: Estimation via Counting Patterns},
    author={Khetan, Ashish and Shah, Harshay and Oh, Sewoong},
    journal={arXiv preprint arXiv:1812.00139},
    year={2018}
}</pre> -->
                <!-- </div> -->
              </div>
            </div>
          </div>
        </div>

        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>
    </div>
  </body>

  <script>
    function highlightDiv(id) {
      const element = document.getElementById(id);
      if (element) {
        element.style.backgroundColor = '';
        setTimeout(() => {
          element.style.backgroundColor = 'rgb(252, 221, 221)';
          setTimeout(() => {
            fadeOutBackground(element, 1500);
          }, 1000);
        }, 10);
      }
    }

    function fadeOutBackground(element, duration) {
      let startTime = null;
      const initialColor = [252, 221, 221];
      const finalColor = [255, 255, 255];

      function animate(timestamp) {
        if (!startTime) startTime = timestamp;
        const progress = Math.min((timestamp - startTime) / duration, 1);

        const r = Math.round(initialColor[0] + progress * (finalColor[0] - initialColor[0]));
        const g = Math.round(initialColor[1] + progress * (finalColor[1] - initialColor[1]));
        const b = Math.round(initialColor[2] + progress * (finalColor[2] - initialColor[2]));

        element.style.backgroundColor = `rgb(${r},${g},${b})`;

        if (progress < 1) {
          requestAnimationFrame(animate);
        }
      }

      requestAnimationFrame(animate);
    }


    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const hash = this.getAttribute('href').substring(1);

        const targetElement = document.getElementById(hash);
        if (targetElement) {
          setTimeout(() => {
            highlightDiv(hash);
          }, 0);
        }
      });
    });

    window.addEventListener('hashchange', () => {
      const hash = window.location.hash.substring(1);
      highlightDiv(hash);
    });

    document.addEventListener('DOMContentLoaded', () => {
      if (window.location.hash) {
        const hash = window.location.hash.substring(1);
        highlightDiv(hash);
      }
    });

  </script>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-55279868-1', 'auto');
    ga('send', 'pageview');
  </script>
</html>
